{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ”§ Merge LoRA Adapter to Full Model\n",
                "\n",
                "This notebook merges your Unsloth LoRA adapter with its base model and uploads the result to Hugging Face.\n",
                "\n",
                "**Before running:**\n",
                "1. Go to Runtime > Change runtime type > Select **T4 GPU**\n",
                "2. Have your Hugging Face **Write Token** ready"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Step 1: Install dependencies\n",
                "!pip install unsloth transformers accelerate bitsandbytes -q"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Step 2: Login to Hugging Face\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Paste your HF token when prompted\n",
                "login()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Step 3: Configuration\n",
                "# =====================\n",
                "# Change these values to match your model\n",
                "\n",
                "ADAPTER_MODEL = \"NurseCitizenDeveloper/relational-intelligence-unsloth-medgemma\"  # Your LoRA adapter\n",
                "OUTPUT_NAME = \"NurseCitizenDeveloper/relational-intelligence-medgemma-merged\"    # Where to save merged model\n",
                "MAX_SEQ_LENGTH = 2048"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Step 4: Load the adapter model\n",
                "from unsloth import FastLanguageModel\n",
                "\n",
                "print(f\"Loading adapter: {ADAPTER_MODEL}\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=ADAPTER_MODEL,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=None,  # Auto-detect\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "\n",
                "print(\"âœ… Model loaded successfully!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Step 5: Merge and push to Hugging Face\n",
                "print(f\"Merging adapter and pushing to: {OUTPUT_NAME}\")\n",
                "\n",
                "model.push_to_hub_merged(\n",
                "    OUTPUT_NAME,\n",
                "    tokenizer,\n",
                "    save_method=\"merged_16bit\",  # Full precision merged model\n",
                ")\n",
                "\n",
                "print(\"ðŸŽ‰ Done! Your merged model is now available at:\")\n",
                "print(f\"https://huggingface.co/{OUTPUT_NAME}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## âœ… Next Steps\n",
                "\n",
                "Once the upload completes:\n",
                "1. Go to your new model page on Hugging Face\n",
                "2. Verify it has a `model.safetensors` file\n",
                "3. Update your PNA Assistant to use the new merged model ID"
            ],
            "metadata": {}
        }
    ]
}